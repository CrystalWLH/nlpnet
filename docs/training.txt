In order to train a SRL or POS model, you need to run train.py and set some parameters. There a lot of options that can be customized for training, and here follows a brief explanation:

Command line arguments
======================

General Options
---------------

-w [NUMBER]			The size of the word window. For SRL, the supplied model used 3, and for POS, 5. It is important to have a reasonably large window in POS so the tagger can analyze the context.
-n [NUMBER]			Number of hidden neurons.
-f [NUMBER]			Generates feature vectors randomly with the given number of dimensions for words. Ignore it if you supply pre-initialized representations.
--load_features		Loads the features vectors representing words. The file containing the data must be set in config.py and be in the data/ directory. Nlpnet uses numpy files for storing representations as 2-dimensional arrays.
-e [NUMBER]			Number of epochs to train the network.
-l [NUMBER]			The learning rate for network weights.
--lf [NUMBER]		The learning rate for all features.
--caps [NUMBER]		Include capitalization as a feature. If a number is given, determine the number of features (default 5).
--suffix [NUMBER]	Same as --caps, but for suffixes. It will use the suffix list present in the data/ directory.
-a [NUMBER]			Stop training when the network achieves this accuracy. Useful to avoid divergence when the learning rate is high.
-v					Verbose mode, it will output more information about what is happening internally.
--load_network		Loads a previously saved network. The file name must be set in config.py and be in the data/ directory. 
--task {srl,pos}	Specify which task to train.
--data [FILENAME]	A file with the training data. It must be in the same format as the files found in the data/corpora/ directory.


SRL
---

-c [NUMBER]			Number of neurons in the convolution layer.
--lt [NUMBER]		Learning rate for the transition table scores.
--pos [NUMBER]		Uses POS as a feature. Currently, it must read the tags from the training data. Works same as --caps.
--chunk [NUMBER]	Uses syntactic chunks as a feature. Same as --pos.
--use_lemma			Reads word lemmas instead of surface forms. It needs to read them from the training data.
--id				Train for argument boundary identification only.
--class				Train for previously identified argument classification only. (if neither this or --id is supplied, trains a network that does both in a single step)
--pred				Train for predicate recognizing only.
--max_dist [NUMBER]				The maximum distance (to predicates and target words) to have an own feature vector. Any distance greater than this will be mapped to a single vector.
--target_features [NUMBER]		Number of features for vectors representing distance to the target word.
--pred_features [NUMBER]		Same as --target_features for the predicate.



Trained Model
-------------

The trained model will consist of several files: one containing the network and one for each feature table (and the distance table, in case of SRL). The file names are specified in config.py. 
